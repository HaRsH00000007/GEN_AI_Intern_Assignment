{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:32:01.283223Z","iopub.execute_input":"2024-11-24T17:32:01.283616Z","iopub.status.idle":"2024-11-24T17:32:02.460348Z","shell.execute_reply.started":"2024-11-24T17:32:01.283578Z","shell.execute_reply":"2024-11-24T17:32:02.459319Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install groq pandas gradio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:32:57.689251Z","iopub.execute_input":"2024-11-24T17:32:57.690056Z","iopub.status.idle":"2024-11-24T17:33:16.102505Z","shell.execute_reply.started":"2024-11-24T17:32:57.690015Z","shell.execute_reply":"2024-11-24T17:33:16.101202Z"}},"outputs":[{"name":"stdout","text":"Collecting groq\n  Downloading groq-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\nCollecting gradio\n  Downloading gradio-5.6.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from groq) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from groq) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from groq) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from groq) (2.9.2)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from groq) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from groq) (4.12.2)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\nCollecting gradio-client==1.4.3 (from gradio)\n  Downloading gradio_client-1.4.3-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: huggingface-hub>=0.25.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.4)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.10.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.3.0)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nCollecting python-multipart==0.0.12 (from gradio)\n  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.2)\nCollecting ruff>=0.2.2 (from gradio)\n  Downloading ruff-0.8.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<1.0,>=0.1.1 (from gradio)\n  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\nCollecting tomlkit==0.12.0 (from gradio)\n  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.3)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.30.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.4.3->gradio) (2024.6.1)\nRequirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.4.3->gradio) (12.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (3.15.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->gradio) (3.1.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (1.26.18)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading groq-0.12.0-py3-none-any.whl (108 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gradio-5.6.0-py3-none-any.whl (57.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.4.3-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\nDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\nDownloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ruff-0.8.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\nInstalling collected packages: tomlkit, semantic-version, ruff, python-multipart, ffmpy, starlette, safehttpx, groq, gradio-client, fastapi, gradio\n  Attempting uninstall: tomlkit\n    Found existing installation: tomlkit 0.13.2\n    Uninstalling tomlkit-0.13.2:\n      Successfully uninstalled tomlkit-0.13.2\n  Attempting uninstall: python-multipart\n    Found existing installation: python-multipart 0.0.9\n    Uninstalling python-multipart-0.0.9:\n      Successfully uninstalled python-multipart-0.0.9\n  Attempting uninstall: starlette\n    Found existing installation: starlette 0.37.2\n    Uninstalling starlette-0.37.2:\n      Successfully uninstalled starlette-0.37.2\n  Attempting uninstall: fastapi\n    Found existing installation: fastapi 0.111.0\n    Uninstalling fastapi-0.111.0:\n      Successfully uninstalled fastapi-0.111.0\nSuccessfully installed fastapi-0.115.5 ffmpy-0.4.0 gradio-5.6.0 gradio-client-1.4.3 groq-0.12.0 python-multipart-0.0.12 ruff-0.8.0 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.12.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import groq\nimport os\nimport re\nimport json\nimport pandas as pd\nimport gradio as gr\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass, asdict\nimport tempfile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:48:08.453321Z","iopub.execute_input":"2024-11-24T17:48:08.454322Z","iopub.status.idle":"2024-11-24T17:48:08.459719Z","shell.execute_reply.started":"2024-11-24T17:48:08.454276Z","shell.execute_reply":"2024-11-24T17:48:08.458531Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"@dataclass\nclass PatientInfo:\n    name: str\n    address: str\n    hospital: str\n    allergies: List[str]\n    major_medical_problems: List[str]\n    sdoh_factors: List[str]\n\n@dataclass\nclass SDOHMatch:\n    factor: str\n    code: str\n    confidence: float\n    explanation: str\n\n@dataclass\nclass OutputFormat:\n    patient_information: Dict\n    sdoh_factors_with_codes: List[Dict]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:48:30.286147Z","iopub.execute_input":"2024-11-24T17:48:30.286600Z","iopub.status.idle":"2024-11-24T17:48:30.294149Z","shell.execute_reply.started":"2024-11-24T17:48:30.286561Z","shell.execute_reply":"2024-11-24T17:48:30.293075Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import os\n\n# Set the GROQ_API_KEY environment variable\nos.environ['GROQ_API_KEY'] = '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'\n\n# Verify it's set\nprint(os.getenv('GROQ_API_KEY'))  # This should print your API key\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:42:51.326291Z","iopub.execute_input":"2024-11-24T17:42:51.327036Z","iopub.status.idle":"2024-11-24T17:42:51.332548Z","shell.execute_reply.started":"2024-11-24T17:42:51.326986Z","shell.execute_reply":"2024-11-24T17:42:51.331433Z"}},"outputs":[{"name":"stdout","text":"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\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def extract_patient_info(clinical_note: str) -> PatientInfo:\n    \"\"\"Extract patient information from clinical note with improved error handling.\"\"\"\n    if not isinstance(clinical_note, str):\n        raise ValueError(\"Clinical note must be a string\")\n\n    # Initialize with default values\n    patient_info = PatientInfo(\n        name=\"\",\n        address=\"\",\n        hospital=\"\",\n        allergies=[],\n        major_medical_problems=[],\n        sdoh_factors=[]\n    )\n    \n    # Extract name\n    name_match = re.search(r\"Pt:[\\s]*([^(\\n]+)\", clinical_note)\n    if name_match:\n        patient_info.name = name_match.group(1).strip()\n    \n    # Extract address\n    address_match = re.search(r\"residing @\\s*(.*?(?:\\d{5}))\", clinical_note)\n    if address_match:\n        patient_info.address = address_match.group(1).strip()\n    \n    # Extract hospital\n    hospital_match = re.search(r\"Treating facility:\\s*(.*?)(?:\\d{5}|\\n)\", clinical_note)\n    if hospital_match:\n        patient_info.hospital = hospital_match.group(1).strip()\n    \n    # Extract allergies\n    allergy_match = re.search(r\"known allergies to:\\s*(.*?)\\.\", clinical_note, re.IGNORECASE)\n    if allergy_match:\n        allergies_text = allergy_match.group(1)\n        patient_info.allergies = [a.strip() for a in allergies_text.split(',') if a.strip()]\n    \n    # Medical problems mapping\n    medical_problem_map = {\n        r\"T2DM\": \"Type 2 Diabetes Mellitus\",\n        r\"COVID\\+\": \"COVID-19\",\n        r\"DM foot ulcer\": \"Diabetic Foot Ulcer\"\n    }\n    \n    medical_problems = []\n    for pattern, condition in medical_problem_map.items():\n        if re.search(pattern, clinical_note, re.IGNORECASE):\n            medical_problems.append(condition)\n    patient_info.major_medical_problems = medical_problems\n    \n    # SDOH factors extraction\n    sdoh_pattern_map = {\n        r\"construction dust/debris\": \"Exposure to dust and smoke\",\n        r\"poor office ventilation\": \"Exposure to environmental pollutants\",\n        r\"2nd hand smoke\": \"Exposure to dust and smoke\",\n        r\"Lives 2nd flr apt w/o elevator\": \"Poor housing conditions\",\n        r\"Poor breakfast compliance\": \"Inadequate nutrition\",\n        r\"relies on wife for transport\": \"Limited access to healthcare services\"\n    }\n    \n    sdoh_factors = []\n    for pattern, factor in sdoh_pattern_map.items():\n        if re.search(pattern, clinical_note, re.IGNORECASE):\n            if factor not in sdoh_factors:  # Avoid duplicates\n                sdoh_factors.append(factor)\n    patient_info.sdoh_factors = sdoh_factors\n    \n    return patient_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:49:22.105352Z","iopub.execute_input":"2024-11-24T17:49:22.106282Z","iopub.status.idle":"2024-11-24T17:49:22.117481Z","shell.execute_reply.started":"2024-11-24T17:49:22.106242Z","shell.execute_reply":"2024-11-24T17:49:22.116473Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def match_sdoh_with_llm(clinical_sdoh: str, reference_factors: List[Dict[str, str]]) -> Dict:\n    \"\"\"Match SDOH factors using Groq LLM.\"\"\"\n    client = groq.Groq(\n        api_key=os.environ.get(\"GROQ_API_KEY\")\n    )\n    \n    prompt = f\"\"\"\n    Task: Match the following clinical SDOH factor with the most appropriate standardized SDOH factor from the reference list.\n    \n    Clinical SDOH factor: \"{clinical_sdoh}\"\n    \n    Reference SDOH factors and codes:\n    {json.dumps(reference_factors, indent=2)}\n    \n    Please provide:\n    1. The best matching standardized SDOH factor\n    2. Its corresponding code\n    3. A confidence score (0-1)\n    4. A brief explanation for the match\n    \n    Format your response as a JSON object with these keys: matched_factor, code, confidence, explanation\n    \"\"\"\n    \n    chat_completion = client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a healthcare coding expert specializing in Social Determinants of Health (SDOH). Your task is to match clinical SDOH descriptions to standardized codes.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ],\n        model=\"mixtral-8x7b-32768\",\n        temperature=0.1,\n        max_tokens=500\n    )\n    \n    try:\n        response_text = chat_completion.choices[0].message.content\n        match_result = json.loads(response_text)\n        return match_result\n    except Exception as e:\n        return {\n            \"matched_factor\": clinical_sdoh,\n            \"code\": \"UNKNOWN\",\n            \"confidence\": 0.0,\n            \"explanation\": f\"Error processing LLM response: {str(e)}\"\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:49:45.261617Z","iopub.execute_input":"2024-11-24T17:49:45.262665Z","iopub.status.idle":"2024-11-24T17:49:45.269786Z","shell.execute_reply.started":"2024-11-24T17:49:45.262623Z","shell.execute_reply":"2024-11-24T17:49:45.268717Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def process_files(clinical_note_file, sdoh_codes_file) -> str:\n    \"\"\"Process uploaded files with LLM-based matching.\"\"\"\n    try:\n        # Validate file inputs\n        if clinical_note_file is None or sdoh_codes_file is None:\n            return json.dumps({\n                \"error\": \"Please upload both a clinical note (TXT) and SDOH codes (CSV) file\"\n            })\n\n        # Read clinical note\n        try:\n            if isinstance(clinical_note_file, str):\n                clinical_note = clinical_note_file\n            else:\n                clinical_note = clinical_note_file.decode('utf-8')\n        except (UnicodeDecodeError, AttributeError):\n            return json.dumps({\n                \"error\": \"Unable to read clinical note. Please ensure it is a valid UTF-8 encoded text file\"\n            })\n\n        # Read SDOH codes\n        try:\n            with tempfile.NamedTemporaryFile(mode='wb', delete=False) as temp_file:\n                if isinstance(sdoh_codes_file, str):\n                    temp_file.write(sdoh_codes_file.encode('utf-8'))\n                else:\n                    temp_file.write(sdoh_codes_file)\n                temp_file_path = temp_file.name\n\n            sdoh_df = pd.read_csv(temp_file_path)\n            os.unlink(temp_file_path)\n\n            required_columns = {'SDOH factor', 'Code'}\n            if not all(col in sdoh_df.columns for col in required_columns):\n                return json.dumps({\n                    \"error\": \"The SDOH codes CSV file must contain 'SDOH factor' and 'Code' columns\"\n                })\n            \n            sdoh_codes_dict = dict(zip(sdoh_df['SDOH factor'], sdoh_df['Code']))\n        except Exception as e:\n            return json.dumps({\n                \"error\": f\"Error reading SDOH codes file: Please ensure it is a valid CSV file with the required columns\",\n                \"details\": str(e)\n            })\n        \n        # Extract and process information with LLM matching\n        patient_info = extract_patient_info(clinical_note)\n        sdoh_matches = match_sdoh_codes(patient_info.sdoh_factors, sdoh_codes_dict)\n        \n        # Format output with enhanced matching information\n        output = OutputFormat(\n            patient_information={\n                \"Name\": patient_info.name,\n                \"Address\": patient_info.address,\n                \"Hospital\": patient_info.hospital,\n                \"Allergies\": patient_info.allergies,\n                \"Major Medical Problems\": patient_info.major_medical_problems\n            },\n            sdoh_factors_with_codes=[{\n                \"original_factor\": match.factor,\n                \"matched_factor\": match.factor,\n                \"code\": match.code,\n                \"confidence\": match.confidence,\n                \"explanation\": match.explanation\n            } for match in sdoh_matches]\n        )\n        \n        return json.dumps(asdict(output), indent=2, ensure_ascii=False)\n        \n    except Exception as e:\n        return json.dumps({\n            \"error\": \"An unexpected error occurred while processing the files\",\n            \"details\": str(e)\n        })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:50:41.932267Z","iopub.execute_input":"2024-11-24T17:50:41.932947Z","iopub.status.idle":"2024-11-24T17:50:41.943724Z","shell.execute_reply.started":"2024-11-24T17:50:41.932906Z","shell.execute_reply":"2024-11-24T17:50:41.942724Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Create Gradio interface\niface = gr.Interface(\n    fn=process_files,\n    inputs=[\n        gr.File(\n            label=\"Upload Clinical Note (TXT file)\",\n            file_types=[\".txt\"],\n            type=\"binary\"\n        ),\n        gr.File(\n            label=\"Upload SDOH Codes (CSV file)\",\n            file_types=[\".csv\"],\n            type=\"binary\"\n        )\n    ],\n    outputs=gr.JSON(label=\"Extracted Information\"),\n    title=\"Clinical Note and SDOH Code Matcher (LLM-Enhanced)\",\n    description=\"\"\"\n    Upload a clinical note (TXT) and SDOH codes (CSV) to extract patient information and match SDOH factors with their corresponding codes.\n    This version uses AI to provide more accurate matching and detailed explanations.\n    \n    The CSV file should contain two columns:\n    - 'SDOH factor': The social determinant of health factor\n    - 'Code': The corresponding diagnostic code\n    \"\"\",\n    allow_flagging=\"never\"\n)\n\nif __name__ == \"__main__\":\n    # Make sure to set your Groq API key\n    if not os.environ.get(\"GROQ_API_KEY\"):\n        print(\"Please set your GROQ_API_KEY environment variable\")\n    else:\n        iface.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T17:51:15.572541Z","iopub.execute_input":"2024-11-24T17:51:15.572925Z","iopub.status.idle":"2024-11-24T17:51:17.739113Z","shell.execute_reply.started":"2024-11-24T17:51:15.572892Z","shell.execute_reply":"2024-11-24T17:51:17.738150Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/gradio/interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7861\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://063b35d88531bb17de.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://063b35d88531bb17de.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}